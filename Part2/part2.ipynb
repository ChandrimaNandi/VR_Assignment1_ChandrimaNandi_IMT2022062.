{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the overlapping splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image split into three overlapping parts.\n"
     ]
    }
   ],
   "source": [
    "def split_image_with_overlap_equal(image_path, overlap=50):\n",
    "    # Open the image\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "\n",
    "    # Calculate the width of each segment so that 3 segments with overlaps exactly span the image width.\n",
    "    # 3p - 2*overlap = width  ==>  p = (width + 2*overlap) / 3\n",
    "    p = int((width + 2 * overlap) / 3)\n",
    "\n",
    "    # Define crop boundaries for each part\n",
    "    part1_left, part1_right = 0, p\n",
    "    part2_left, part2_right = p - overlap, (p - overlap) + p\n",
    "    part3_left, part3_right = 2 * p - 2 * overlap, (2 * p - 2 * overlap) + p\n",
    "\n",
    "    # Crop each part of the image\n",
    "    part1 = img.crop((part1_left, 0, part1_right, height))\n",
    "    part2 = img.crop((part2_left, 0, part2_right, height))\n",
    "    part3 = img.crop((part3_left, 0, part3_right, height))\n",
    "\n",
    "    # Create the 'input' directory if it doesn't exist\n",
    "    output_dir = \"input\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the resulting image_path in the 'input' directory\n",
    "    part1.save(os.path.join(output_dir, \"part1.jpg\"))\n",
    "    part2.save(os.path.join(output_dir, \"part2.jpg\"))\n",
    "    part3.save(os.path.join(output_dir, \"part3.jpg\"))\n",
    "    \n",
    "    print(\"Image split into three overlapping parts.\")\n",
    "\n",
    "split_image_with_overlap_equal(\"pano.jpg\", overlap=900)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ORB extractor\n",
    "feature_extractor = cv2.ORB_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Image and Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_extract_features(image_file):\n",
    "    \"\"\"Load an image and extract ORB keypoints and descriptors.\"\"\"\n",
    "    img = cv2.imread(image_file)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    keypoints, descriptors = feature_extractor.detectAndCompute(gray, None)\n",
    "    return img, keypoints, descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stitch Two Images Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_pair(img1, img2, kp1, kp2, desc1, desc2):\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING) # Create a Brute Force matcher\n",
    "    knn_matches = matcher.knnMatch(desc1, desc2, k=2) # Find the 2 best matches for each descriptor\n",
    "    good_matches = [m for m, n in knn_matches if m.distance < 0.75 * n.distance]\n",
    "    # Check if we found enough good matches\n",
    "    if len(good_matches) < 4:\n",
    "        print(\"Not enough matches to compute homography.\")\n",
    "        return None\n",
    "    # Extract location of good matches\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches])\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches])\n",
    "    # Compute homography\n",
    "    H, _ = cv2.findHomography(pts2, pts1, cv2.RANSAC, 5.0)\n",
    "    if H is None:\n",
    "        print(\"Homography computation failed.\")\n",
    "        return None\n",
    "    # Warp img2 to img1's perspective\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "    # Compute the corners of the image to be warped\n",
    "    corners_img2 = np.float32([[0, 0], [w2, 0], [w2, h2], [0, h2]]).reshape(-1, 1, 2)\n",
    "    warped_corners = cv2.perspectiveTransform(corners_img2, H)\n",
    "    corners_img1 = np.float32([[0, 0], [w1, 0], [w1, h1], [0, h1]]).reshape(-1, 1, 2)\n",
    "    all_corners = np.concatenate((corners_img1, warped_corners), axis=0)\n",
    "    # Compute the bounding box of the merged image\n",
    "    xmin, ymin = np.int32(all_corners.min(axis=0).ravel() - 0.5)\n",
    "    xmax, ymax = np.int32(all_corners.max(axis=0).ravel() + 0.5)\n",
    "    # Compute the translation to be applied to img2\n",
    "    trans = [-xmin, -ymin]\n",
    "    T = np.array([[1, 0, trans[0]],\n",
    "                  [0, 1, trans[1]],\n",
    "                  [0, 0, 1]])\n",
    "    # Merge the two images\n",
    "    merged = cv2.warpPerspective(img2, T.dot(H), (xmax - xmin, ymax - ymin))\n",
    "    merged[trans[1]:trans[1] + h1, trans[0]:trans[0] + w1] = img1\n",
    "    # Remove black borders by cropping\n",
    "    gray_merged = cv2.cvtColor(merged, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary = cv2.threshold(gray_merged, 1, 255, cv2.THRESH_BINARY)\n",
    "    nonzero = cv2.findNonZero(binary)\n",
    "    # Compute the bounding box of the non-black region\n",
    "    x, y, w, h = cv2.boundingRect(nonzero)\n",
    "    # Crop the image to the bounding box \n",
    "    return merged[y:y+h, x:x+w]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Annotated Keypoint Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_annotated_keypoints(image_paths, img_list, kp_list, output_dir=\"output\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Save the images with annotated keypoints\n",
    "    for path, img, kps in zip(image_paths, img_list, kp_list):\n",
    "        annotated = cv2.drawKeypoints(img, kps, None, color=(0, 255, 0))\n",
    "        out_filename = os.path.join(output_dir, \"keypoints_\" + os.path.basename(path))\n",
    "        cv2.imwrite(out_filename, annotated)\n",
    "        print(\"Saved annotated image:\", out_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refine the Final Panorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_panorama(panorama, save_path):\n",
    "    # Add a black border to the image to avoid errors when finding contours\n",
    "    bordered = cv2.copyMakeBorder(panorama, 5, 5, 5, 5, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "    # Convert the image to grayscale and find contours\n",
    "    gray = cv2.cvtColor(bordered, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply a binary threshold to get a black and white image\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)\n",
    "    # Find the largest contour\n",
    "    contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Extract the contours and find the largest one\n",
    "    contours = imutils.grab_contours(contours)\n",
    "    # Find the largest contour\n",
    "    largest = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(largest)\n",
    "    \n",
    "    # Create and refine a mask to tightly crop the image\n",
    "    mask = np.zeros(thresh.shape, dtype=\"uint8\")\n",
    "    cv2.rectangle(mask, (x, y), (x + w, y + h), 255, -1)\n",
    "    refined_mask = mask.copy()\n",
    "    diff = mask.copy()\n",
    "    # Erode the mask until the difference is zero\n",
    "    while cv2.countNonZero(diff) > 0:\n",
    "        refined_mask = cv2.erode(refined_mask, None)\n",
    "        diff = cv2.subtract(refined_mask, thresh)\n",
    "    # Find contours in the refined mask\n",
    "    contours_refined = cv2.findContours(refined_mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_refined = imutils.grab_contours(contours_refined)\n",
    "    # Find the largest contour in the refined mask\n",
    "    best = max(contours_refined, key=cv2.contourArea)\n",
    "    # Crop the image using the bounding box of the largest contour\n",
    "    X, Y, W, H = cv2.boundingRect(best)\n",
    "    final_crop = bordered[Y:Y+H, X:X+W]\n",
    "    # Return the cropped image\n",
    "    return final_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved annotated image: output/keypoints_part1.jpg\n",
      "Saved annotated image: output/keypoints_part2.jpg\n",
      "Saved annotated image: output/keypoints_part3.jpg\n",
      "Refined panorama saved.\n"
     ]
    }
   ],
   "source": [
    "# Get list of .jpg images from the \"input\" folder\n",
    "input_dir = \"input\"\n",
    "image_paths = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.lower().endswith('.jpg')]\n",
    "\n",
    "# Load images and extract features\n",
    "img_list, kp_list, desc_list = [], [], []\n",
    "for file in image_paths:\n",
    "    img, kps, desc = load_and_extract_features(file) # Load and extract features\n",
    "    img_list.append(img) # Append to the list of images\n",
    "    kp_list.append(kps) # Append to the list of keypoints\n",
    "    desc_list.append(desc) # Append to the list of descriptors\n",
    "\n",
    "# Save annotated images with keypoints\n",
    "save_annotated_keypoints(image_paths, img_list, kp_list)\n",
    "\n",
    "# Begin stitching with the first two images\n",
    "panorama = stitch_pair(img_list[0], img_list[1], kp_list[0], kp_list[1], desc_list[0], desc_list[1])\n",
    "if panorama is None:\n",
    "    raise Exception(\"Initial stitching failed.\")\n",
    "\n",
    "# Iteratively stitch remaining images\n",
    "for i in range(2, len(img_list)):\n",
    "    # Recompute keypoints on the current panorama for robust merging\n",
    "    gray_panorama = cv2.cvtColor(panorama, cv2.COLOR_BGR2GRAY)\n",
    "    # Detect keypoints and compute descriptors\n",
    "    new_kps, new_desc = feature_extractor.detectAndCompute(gray_panorama, None)\n",
    "    # Stitch the new image with the current panorama\n",
    "    panorama = stitch_pair(panorama, img_list[i], new_kps, kp_list[i], new_desc, desc_list[i])\n",
    "    if panorama is None:\n",
    "        print(\"Stitching failed at image index\", i)\n",
    "        break\n",
    "\n",
    "# Save the complete stitched panorama\n",
    "cv2.imwrite(\"output/stitched_pano.jpg\", panorama)\n",
    "\n",
    "# Refine and crop the final panorama\n",
    "refined_panorama = crop_panorama(panorama, os.path.join(\"output\", \"stitched_pano.jpg\"))\n",
    "print(\"Refined panorama saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
